{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328b69fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78fe1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI API key and Qdrant client\n",
    "openai.api_key = ''\n",
    "qdrant_client = QdrantClient(host='localhost', port=6333)\n",
    "\n",
    "# Function to retrieve relevant documents from Qdrant\n",
    "def retrieve_relevant_documents(query, collection_name='umrah_guides_2', limit=10):\n",
    "    embeddings_model = OpenAIEmbeddings(api_key=openai.api_key)\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "#     print(\"Query Embedding:\", query_embedding)  # Debug print\n",
    "    \n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=limit\n",
    "    )\n",
    "#     print(\"\\nRaw Search Results:\", search_result)  # Debug print\n",
    "    \n",
    "    relevant_texts = [result.payload.get(\"text\", \"\") for result in search_result]\n",
    "#     print(\"\\nRelevant Texts:\", relevant_texts)  # Debug print\n",
    "    \n",
    "    return relevant_texts\n",
    "\n",
    "# Function to re-rank documents using BM25\n",
    "def re_rank_documents(query, documents):\n",
    "    tokenized_docs = [doc.split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    tokenized_query = query.split()\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    ranked_documents = [doc for _, doc in sorted(zip(doc_scores, documents), reverse=True)]\n",
    "    \n",
    "    # Debugging: Check ranked documents\n",
    "#     print(\"--\"*40, \"\\nRanked Documents:\", ranked_documents)\n",
    "    return ranked_documents\n",
    "\n",
    "# Function to generate response using fine-tuned model with retry mechanism\n",
    "def generate_response(query, relevant_texts, fine_tuned_model, max_tokens=1000, max_retries=5, delay=10):\n",
    "    context = \"\\n\\n\".join(relevant_texts)\n",
    "    context_tokens = context.split()\n",
    "    if len(context_tokens) > max_tokens:\n",
    "        context = \" \".join(context_tokens[:max_tokens])\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant with expertise in Umrah & Hajj. You will get a query and a context. If the context is relevant with the query, use it to answer. if not, IGNORE the context and just answer and always explain your answer! If the query is not related to hajj and umrah, respectfully say that it is outside your expertise!\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        {\"role\": \"assistant\", \"content\": context}\n",
    "    ]\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Generate the response using the fine-tuned model\n",
    "            response = openai.chat.completions.create(\n",
    "                model=fine_tuned_model,\n",
    "                messages=messages,\n",
    "                max_tokens=2048,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except openai.RateLimitError as e:\n",
    "            print(\"A 429 status code was received; we should back off a bit.\")\n",
    "            time.sleep(delay)\n",
    "            retries += 1\n",
    "            delay *= 2  # Exponential backoff\n",
    "        except openai.APIConnectionError as e:\n",
    "            print(\"The server could not be reached\")\n",
    "            print(e.__cause__)\n",
    "            return None\n",
    "        except openai.APIStatusError as e:\n",
    "            print(f\"Another non-200-range status code was received: {e.status_code}\")\n",
    "            print(e.response)\n",
    "            return None\n",
    "        except openai.APIError as e:\n",
    "            print(f\"OpenAI error: {e}\")\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd826bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response: Untuk perjalanan umrah minggu depan, hal-hal yang perlu disiapkan antara lain: persiapkan fisik dengan olahraga ringan, bawa perlengkapan shalat, baju ganti, obat-obatan pribadi, serta dokumen penting. Pastikan juga telah memahami tata cara umrah agar ibadah berjalan lancar. Jika ada hal lain yang ingin ditanyakan terkait persiapan umrah, silakan disampaikan!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    query = \"Minggu depan berangkat umrah nih, apa aja ya yang perlu disiapin?\"\n",
    "    initial_relevant_texts = retrieve_relevant_documents(query)\n",
    "    re_ranked_texts = re_rank_documents(query, initial_relevant_texts)\n",
    "    response = generate_response(query, re_ranked_texts, fine_tuned_model='ft:gpt-3.5-turbo-0125:personal:umroh-ai-mvp-29jul:9qAxDEDg')\n",
    "    print(\"\\nGenerated Response:\", response)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505111d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (umrah_env)",
   "language": "python",
   "name": "umrah_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
