{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "854d39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c7364a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 50\n",
      "First example:\n",
      "{'role': 'system', 'content': 'This assistant is knowledgeable about Wukuf, Umrah preparation, and other rituals details'}\n",
      "{'role': 'assistant', 'content': \"Assalamu'alaikum! Bagaimana kabar Bapak/Ibu? Apa ada yang bisa saya bantu terkait persiapan Umrah atau pertanyaan lainnya?\"}\n",
      "{'role': 'user', 'content': \"Wa'alaikumsalam. Alhamdulillah, saya baik. Saya ingin tahu, bagaimana sih cara melaksanakan wukuf di Arafah?\"}\n",
      "{'role': 'assistant', 'content': 'Alhamdulillah, senang mendengarnya. Wukuf di Arafah adalah puncak ibadah haji yang dilakukan dengan berdiam diri di Padang Arafah pada 9 Dzulhijjah, mulai dari tergelincir matahari sampai terbenam. Wukuf ini sangat penting karena tanpa melaksanakannya, haji tidak sah. Ada hal lain tentang wukuf di Arafah yang ingin Bapak/Ibu ketahui?'}\n",
      "{'role': 'user', 'content': 'Terima kasih informasinya. Kalau misalnya saya sakit selama umrah, apa yang harus saya lakukan?'}\n",
      "{'role': 'assistant', 'content': 'Jika Bapak/Ibu sakit selama umrah, sebaiknya segera mencari bantuan medis. Di sekitar Masjidil Haram dan Masjid Nabawi terdapat fasilitas kesehatan yang siap membantu jamaah. Apakah ada pertanyaan lain terkait kesehatan selama umrah?'}\n",
      "{'role': 'user', 'content': 'Ya, mengenai makanan selama umrah. Apa saja makanan yang direkomendasikan?'}\n",
      "{'role': 'assistant', 'content': 'Makanan yang direkomendasikan selama umrah adalah makanan yang higienis, bergizi, dan seimbang. Konsumsi makanan seperti buah-buahan, sayuran, protein, dan cukup minum air. Hindari makanan yang terlalu berat atau berminyak. Apakah ada makanan tertentu yang ingin bapak/ibu tanyakan?'}\n",
      "{'role': 'user', 'content': 'Terima kasih. Oh ya, bagaimana cara menghindari penipuan selama umrah?'}\n",
      "{'role': 'assistant', 'content': 'Halo bapak/ibu, untuk menghindari penipuan selama umrah, pastikan selalu membeli barang dari tempat resmi dan waspadai orang yang menawarkan bantuan dengan imbalan. Apakah bapak/ibu butuh tips lainnya?'}\n",
      "{'role': 'user', 'content': 'Saya dengar tentang haji ifrad. Apa itu haji ifrad?'}\n",
      "{'role': 'assistant', 'content': 'Haji ifrad adalah pelaksanaan ibadah haji tanpa umrah, dengan niat ihram hanya untuk haji sejak awal. Apakah bapak/ibu ingin penjelasan lebih lanjut tentang perbedaan haji ifrad dengan jenis haji lainnya?'}\n",
      "{'role': 'user', 'content': 'Menarik juga, terima kasih. Bagaimana dengan ihram dari miqat? Apa itu?'}\n",
      "{'role': 'assistant', 'content': 'Ihram dari miqat adalah memulai niat umroh dari tempat yang telah ditentukan sesuai dengan asal jamaah. Miqat adalah tempat atau waktu di mana jamaah harus mulai berihram. Apakah bapak/ibu ingin mengetahui lebih rinci tentang miqat?'}\n",
      "{'role': 'user', 'content': 'Iya, sekalian dong. Apa yang dimaksud dengan niat ihram?'}\n",
      "{'role': 'assistant', 'content': 'Niat ihram adalah niat untuk memulai ibadah haji atau umroh yang dilakukan saat berihram dari miqat. Niat ini adalah salah satu rukun haji yang harus dilakukan agar haji sah. Apakah bapak/ibu ingin mengetahui lebih lanjut tentang niat ihram?'}\n",
      "{'role': 'user', 'content': \"Lalu, apa langkah pertama dalam haji tamatu'?\"}\n",
      "{'role': 'assistant', 'content': \"Baik bapak dan ibu, langkah pertama dalam haji tamatu' adalah ihram dari miqat. Miqat adalah tempat yang telah ditentukan untuk memulai ihram. Di sini, kita berniat untuk umrah dan mengucapkan talbiyah, yaitu 'Labbaik Allahumma labbaik.' Dengan niat dan talbiyah ini, kita memasuki keadaan ihram, di mana kita harus mematuhi larangan-larangan ihram. Setelah itu, kita melanjutkan perjalanan menuju Mekkah. Apakah bapak/ibu ingin mengetahui lebih rinci tentang miqat dan tata cara ihram?\"}\n",
      "{'role': 'user', 'content': 'Bagaimana dengan sunnah-sunnah dalam ibadah haji?'}\n",
      "{'role': 'assistant', 'content': 'Sunnah dalam ibadah haji antara lain: Mandi saat hendak ihram, mengenakan pakaian ihram yang terdiri dari dua lembar, talbiyah dengan suara keras, mabit di Mina pada malam Arafah, mencium hajar aswad, idhthiba’ saat thawaf qudum atau thawaf umrah, dan raml pada tiga putaran pertama thawaf qudum atau thawaf umrah. Tidak ada resiko apapun bagi yang meninggalkan sunnah ini. Apakah Anda ingin mengetahui lebih lanjut tentang sunnah haji?'}\n",
      "{'role': 'user', 'content': 'Doa apa yang dianjurkan saat meminum air zamzam?'}\n",
      "{'role': 'assistant', 'content': \"Saat meminum air zamzam, dianjurkan untuk berdoa meminta kebaikan dunia dan akhirat. Doa yang sering dibaca adalah 'Allahumma inni as-aluka ilman naafi’an wa rizqan waasi’an wa shifaa’an min kulli daa’' yang berarti 'Ya Allah, aku mohon ilmu yang bermanfaat, rezeki yang luas, dan kesembuhan dari segala penyakit.' Apakah Anda ingin mengetahui lebih banyak doa yang dianjurkan saat meminum air zamzam?\"}\n",
      "{'role': 'user', 'content': 'Saya juga ingin tahu, apa yang harus dilakukan setelah selesai thawaf?'}\n",
      "{'role': 'assistant', 'content': 'Setelah selesai thawaf, disunnahkan untuk shalat dua rakaat di belakang maqam Ibrahim, kemudian meminum air zamzam dan membasahi tubuh dengannya. Setelah itu, menuju bukit Shafa untuk memulai sa’i. Apakah ada hal lain yang ingin Anda ketahui tentang rangkaian ibadah setelah thawaf?'}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"50_conversation_json_file.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78f7087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0235b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2f87714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 24, 24\n",
      "mean / median: 24.0, 24.0\n",
      "p5 / p95: 24.0, 24.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 1484, 1484\n",
      "mean / median: 1484.0, 1484.0\n",
      "p5 / p95: 1484.0, 1484.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 1122, 1122\n",
      "mean / median: 1122.0, 1122.0\n",
      "p5 / p95: 1122.0, 1122.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2286bf",
   "metadata": {},
   "source": [
    "### Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6e7ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~74200 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~222600 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58808023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Total Cost For Training Using GPT3.5: $ 1.7808\n",
      "Estimated Total Cost For Training Using Davinci: $ 1.3356000000000001\n",
      "Estimated Total Cost For Training Using Cost Babbage: $ 0.08904000000000001\n"
     ]
    }
   ],
   "source": [
    "# Prices per million tokens for each model\n",
    "price_per_million_gpt35 = 8.00\n",
    "price_per_million_davinci = 6.00\n",
    "price_per_million_babbage = 0.40\n",
    "\n",
    "# Total tokens charged during training\n",
    "tokens_charged = n_epochs * n_billing_tokens_in_dataset\n",
    "\n",
    "# Calculate cost per token for each model\n",
    "cost_per_token_gpt35 = price_per_million_gpt35 / 1000000\n",
    "cost_per_token_davinci = price_per_million_davinci / 1000000\n",
    "cost_per_token_babbage = price_per_million_babbage / 1000000\n",
    "\n",
    "# Calculate total training cost for each model\n",
    "total_cost_gpt35 = cost_per_token_gpt35 * tokens_charged\n",
    "total_cost_davinci = cost_per_token_davinci * tokens_charged\n",
    "total_cost_babbage = cost_per_token_babbage * tokens_charged\n",
    "\n",
    "print('Estimated Total Cost For Training Using GPT3.5: $', total_cost_gpt35)\n",
    "print('Estimated Total Cost For Training Using Davinci: $', total_cost_davinci)\n",
    "print('Estimated Total Cost For Training Using Cost Babbage: $', total_cost_babbage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fae0b5",
   "metadata": {},
   "source": [
    "## Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ff5c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4817e7cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='')\n",
    "\n",
    "training_file = client.files.create(\n",
    "  file=open(\"50_conversation_json_file.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a6d0545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-OoeIlkAly7hfozZJZDrh9PN9\n"
     ]
    }
   ],
   "source": [
    "print(training_file.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5aa83075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Fine-Tuning Job\n",
    "suffix_name = \"umroh_ai_mvp_29jul\"\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file.id,\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    suffix=suffix_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7693baa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-OoeIlkAly7hfozZJZDrh9PN9'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b127acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'validating_files'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51855a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (umrah_env)",
   "language": "python",
   "name": "umrah_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
